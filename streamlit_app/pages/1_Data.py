"""
DonnÃ©es & StratÃ©gie d'EntraÃ®nement

Overview of the database state and explanation of the training approach.
"""
import streamlit as st
import sys
from pathlib import Path
import pandas as pd
import psycopg2
from psycopg2.extras import RealDictCursor
import plotly.express as px

# Add paths
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
streamlit_app_root = Path(__file__).parent.parent
sys.path.insert(0, str(streamlit_app_root))

from utils.env_config import get_db_config

# Page configuration
st.set_page_config(
    page_title="Data - Rakuten MLOps",
    page_icon="ðŸ—„ï¸",
    layout="wide",
)

st.title("DonnÃ©es & StratÃ©gie d'EntraÃ®nement")

DB_CONFIG = get_db_config()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Section 1 : Ã‰tat de la base de donnÃ©es
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.header("Ã‰tat de la base de donnÃ©es")


@st.cache_data(ttl=30)
def get_database_stats():
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor(cursor_factory=RealDictCursor)

        cur.execute("""
            SELECT COUNT(*) as total_products,
                   COUNT(DISTINCT l.prdtypecode) as total_classes
            FROM products p
            LEFT JOIN labels l ON p.productid = l.productid
        """)
        stats = cur.fetchone()

        cur.execute("""
            SELECT percentage, total_rows, completed_at
            FROM data_loads WHERE status = 'completed'
            ORDER BY percentage DESC LIMIT 1
        """)
        load_info = cur.fetchone()

        cur.close()
        conn.close()

        return {
            "total_products": stats["total_products"] if stats else 0,
            "total_classes": stats["total_classes"] if stats else 0,
            "current_percentage": float(load_info["percentage"]) if load_info else 0,
            "last_load_date": load_info["completed_at"] if load_info else None,
        }
    except Exception as e:
        st.error(f"Connexion impossible : {e}")
        return None


stats = get_database_stats()

if stats:
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("Produits en base", f"{stats['total_products']:,}")
    c2.metric("Classes", stats["total_classes"])
    c3.metric("DonnÃ©es chargÃ©es", f"{stats['current_percentage']:.0f} %")
    c4.metric("Prochain chargement", f"{min(stats['current_percentage'] + 3, 100):.0f} %")

    st.progress(stats["current_percentage"] / 100)
    if stats["last_load_date"]:
        st.caption(f"Dernier chargement : {stats['last_load_date'].strftime('%Y-%m-%d %H:%M')}")
else:
    st.warning("Base de donnÃ©es non disponible. Lancer `make init-db`.")

st.markdown("")

# â”€â”€ Class distribution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@st.cache_data(ttl=30)
def get_class_distribution():
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        df = pd.read_sql_query(
            """
            SELECT l.prdtypecode, COUNT(*) as count
            FROM labels l JOIN products p ON p.productid = l.productid
            GROUP BY l.prdtypecode ORDER BY count DESC
            """,
            conn,
        )
        conn.close()
        return df
    except Exception:
        return None


dist_df = get_class_distribution()

if dist_df is not None and len(dist_df) > 0:
    # Sort by count ascending so largest bar is at the top
    plot_df = dist_df.sort_values("count", ascending=True)
    plot_df["prdtypecode"] = plot_df["prdtypecode"].astype(str)

    fig = px.bar(
        plot_df,
        x="count",
        y="prdtypecode",
        orientation="h",
        title="Distribution des classes (donnÃ©es brutes)",
        labels={"prdtypecode": "Code catÃ©gorie", "count": "Nombre de produits"},
    )
    fig.update_traces(marker_color="#1f77b4")
    fig.update_layout(yaxis=dict(type="category"))
    st.plotly_chart(fig, use_container_width=True)

    imbalance = dist_df["count"].max() / dist_df["count"].min() if dist_df["count"].min() > 0 else 0
    st.caption(
        f"{len(dist_df)} classes â€” ratio de dÃ©sÃ©quilibre : **{imbalance:.1f}x** "
        f"(classe la plus frÃ©quente / la plus rare)"
    )

# â”€â”€ Load history â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@st.cache_data(ttl=30)
def get_load_history():
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        df = pd.read_sql_query(
            """
            SELECT batch_name, percentage, total_rows, completed_at
            FROM data_loads WHERE status = 'completed'
            ORDER BY percentage ASC
            """,
            conn,
        )
        conn.close()
        return df
    except Exception:
        return None


history_df = get_load_history()

if history_df is not None and len(history_df) > 0:
    st.subheader("Historique des chargements")
    display = history_df.copy()
    display["completed_at"] = pd.to_datetime(display["completed_at"]).dt.strftime("%Y-%m-%d %H:%M")
    display["percentage"] = display["percentage"].apply(lambda x: f"{x:.0f} %")
    display["total_rows"] = display["total_rows"].apply(lambda x: f"{x:,}")
    st.dataframe(
        display,
        use_container_width=True,
        hide_index=True,
        column_config={
            "batch_name": "Batch",
            "percentage": "ChargÃ©",
            "total_rows": "Lignes",
            "completed_at": "Date",
        },
    )

st.markdown("---")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Section 2 : StratÃ©gie d'entraÃ®nement
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.header("StratÃ©gie d'entraÃ®nement")

st.markdown("""
**Chargement incrÃ©mental (+3 % / semaine)**
Les donnÃ©es sont chargÃ©es progressivement de 40 % Ã  100 % pour simuler un flux rÃ©el
de nouvelles donnÃ©es en production. Chaque chargement est tracÃ© dans une table d'audit
PostgreSQL, garantissant la reproductibilitÃ© complÃ¨te de chaque entraÃ®nement.

**ModÃ¨le : TF-IDF + Logistic Regression**
Choix pragmatique pour un pipeline de classification texte : rapide Ã  entraÃ®ner,
facile Ã  interprÃ©ter, et suffisamment performant pour un grand nombre de classes.
Les features sont extraites via TF-IDF (unigrammes + bigrammes, 5 000 features max),
puis classifiÃ©es par une Logistic Regression rÃ©gularisÃ©e.

**RÃ©Ã©quilibrage par RandomOverSampling**
Le dataset original est fortement dÃ©sÃ©quilibrÃ© (ratio jusqu'Ã  ~30x).
Avant chaque entraÃ®nement, un sur-Ã©chantillonnage alÃ©atoire ramÃ¨ne toutes les classes
au mÃªme effectif, Ã©vitant que le modÃ¨le ignore les classes minoritaires.
Le dataset rÃ©Ã©quilibrÃ© est loggÃ© dans MLflow comme artifact pour traÃ§abilitÃ©.
""")
