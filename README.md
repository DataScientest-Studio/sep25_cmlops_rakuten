# Rakuten MLOps Pipeline - Certification Project

A complete MLOps pipeline for product classification with incremental data loading, experiment tracking, model serving, and monitoring.

**Project**: DataScientest MLOps Certification (September 2025)

---

## ğŸ¯ Overview

This project demonstrates a production-ready MLOps pipeline featuring:

- **Incremental Data Pipeline**: PostgreSQL database with audit trail (40% â†’ 100% data progression)
- **Experiment Tracking**: MLflow for experiment versioning and model registry
- **Model Serving**: FastAPI service with health monitoring
- **Monitoring Stack**: Prometheus metrics + Grafana dashboards
- **Interactive UI**: Streamlit control room for pipeline management
- **Complete Versioning**: Database audit trail tracks all data changes for reproducibility

### Key MLOps Capabilities Demonstrated

âœ… **Data Versioning**: Database tracks every data load with timestamps and batch IDs  
âœ… **Experiment Tracking**: MLflow logs all training runs, parameters, and metrics  
âœ… **Model Registry**: Versioned models with stage promotion (Staging â†’ Production)  
âœ… **Model Serving**: REST API with automatic model reloading  
âœ… **Monitoring**: Prometheus metrics + Grafana dashboards for drift detection  
âœ… **Reproducibility**: Complete lineage from data version â†’ training â†’ model â†’ predictions

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit UI   â”‚  â† Interactive control room
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Pipeline Orchestration (Manual)    â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚PostgreSQL â”‚  â”‚  MLflow   â”‚  â”‚  MinIO   â”‚
    â”‚ (+ Audit) â”‚  â”‚ Tracking  â”‚  â”‚ Storage  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚               â”‚              â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚     FastAPI Model Serving (API)         â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Prometheus   â”‚  â”‚  Grafana  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Services**:
- **PostgreSQL**: Data storage with complete audit trail
- **MinIO**: S3-compatible object storage for MLflow artifacts
- **MLflow**: Experiment tracking and model registry
- **FastAPI**: Model serving API with Prometheus metrics
- **Prometheus**: Metrics collection and alerting
- **Grafana**: Visualization dashboards
- **Streamlit**: Interactive control room UI

---

## ğŸš€ Quick Start

### Prerequisites

- Docker Desktop running
- Training data in `data/raw/` (X_train.csv, Y_train.csv, X_test.csv)
- Python 3.11+ (for local Streamlit)

### 1. Initial Setup

```bash
# Clone and enter repository
cd sep25_cmlops_rakuten

# Setup environment and directories
make setup

# Edit .env file with your credentials (optional, defaults are fine for local)
```

### 2. Start All Services

```bash
# Start complete stack: PostgreSQL, MLflow, MinIO, API, Monitoring
make start

# Wait ~30 seconds for services to initialize
```

**Service URLs**:
- MLflow UI: http://localhost:5000
- API Docs: http://localhost:8000/docs
- Prometheus: http://localhost:9090
- Grafana: http://localhost:3000 (admin/admin)

### 3. Initialize Database (40% of data)

```bash
# Load initial 40% of training data into PostgreSQL
make init-db

# âœ… This loads ~33,966 products with complete audit trail
```

### 4. Launch Streamlit Control Room

```bash
# Install Streamlit dependencies
make install-streamlit

# Launch the control room
make run-streamlit
```

Open http://localhost:8501 in your browser.

---

## ğŸ“Š Using the Streamlit Control Room

The Streamlit interface provides 4 pages for the complete ML pipeline:

### Page 1: ğŸ“Š Database Pipeline
- View current data loading status (40% â†’ 100%)
- Monitor class distribution
- Track data loading history
- View sample products

### Page 2: ğŸ”„ Ingestion & Training
- **Load More Data**: Click button to load next 3% increment
- **Generate Balanced Dataset**: Create training dataset with oversampling
- **Train Model**: Configure and train TF-IDF + LogisticRegression
- View MLflow experiments and training metrics
- Explore model artifacts

### Page 3: ğŸš€ Model Promotion & Prediction
- View registered models and versions
- **Promote Models**: Move models between stages (Staging â†’ Production)
- **Test Predictions**: Send test requests to the API
- Monitor API health

### Page 4: ğŸ“ˆ Drift & Monitoring
- View Grafana dashboards
- Check Prometheus metrics
- Analyze inference logs
- Monitor system health

---

## ğŸ“‹ Common Commands

```bash
# === Infrastructure ===
make start              # Start all services
make stop               # Stop all services
make restart            # Restart all services
make ps                 # Show running containers
make check-health       # Check service health

# === Data Pipeline ===
make init-db            # Initialize with 40% data
make load-data          # Load next +3% increment
make status             # Show current data status
make generate-dataset   # Generate balanced dataset

# === Model Training ===
make train-model        # Train model from database
make train-model-promote # Train + auto-promote if F1 > 0.70

# === Monitoring ===
make logs               # View all logs
make logs-api           # View API logs
make logs-mlflow        # View MLflow logs
make test-api           # Test API endpoints

# === Development ===
make run-streamlit      # Launch Streamlit UI
make install-local      # Install Python dependencies
make shell-postgres     # Open PostgreSQL shell

# === Quick Demo ===
make demo               # Complete setup for demo (setup + start + init-db)
```

---

## ğŸ”„ Complete Workflow Example

### Scenario: Train a Model on 40% Data, Promote, and Predict

```bash
# 1. Start services and initialize database
make demo

# 2. Launch Streamlit
make run-streamlit

# 3. In Streamlit (Page 2 - Ingestion & Training):
#    - Click "Generate Balanced Dataset"
#    - Click "Train Model" (configure as needed)
#    - Wait for training to complete (~2-3 minutes)

# 4. In Streamlit (Page 3 - Model Promotion):
#    - View the new model version
#    - Promote to "Production"
#    - Test prediction with sample text

# 5. View in MLflow UI (http://localhost:5000):
#    - Check experiment runs
#    - View metrics and artifacts
#    - Compare model versions

# 6. Monitor API (Page 4 - Drift Monitoring):
#    - View inference logs
#    - Check Grafana dashboards
#    - Monitor prediction distribution
```

---

## ğŸ—„ï¸ Data Versioning & Reproducibility

### How Versioning Works

The pipeline uses **PostgreSQL audit tables** for complete data lineage:

```sql
-- data_loads: Tracks each data loading batch
- batch_name (e.g., "week_1", "week_2")
- percentage (40%, 43%, 46%...)
- total_rows
- started_at, completed_at
- metadata (JSON with context)

-- products_history: Audit trail of all changes
- operation_type (INSERT/UPDATE)
- operation_date
- load_batch_id (links to data_loads)
- product details (designation, description, etc.)
```

### To Reproduce a Training

Given an MLflow run_id, you can reproduce the exact training:

```python
# 1. Get training metadata from MLflow
run = mlflow.get_run(run_id)
training_date = run.info.start_time

# 2. Query database for exact data state at that time
SELECT * FROM products WHERE created_at <= training_date

# 3. Use same hyperparameters from MLflow
params = run.data.params  # max_features, C, ngram_range, etc.

# 4. Retrain with identical setup
```

**No external versioning tools needed!** The database audit trail provides complete lineage.

---

## ğŸ“ˆ Model Training

### TF-IDF + Logistic Regression Pipeline

```python
# What gets trained:
- Text Preprocessing (clean, lowercase, remove special chars)
- TF-IDF Vectorization (max_features=5000, ngram_range=(1,2))
- Logistic Regression (C=1.0, max_iter=1000)

# Tracked in MLflow:
- All hyperparameters
- Metrics: accuracy, F1, precision, recall, per-class metrics
- Artifacts: model pipeline, TF-IDF vectorizer, confusion matrix
- Data version: batch_id and percentage
```

### Training Methods

**Option 1: Via Streamlit** (Recommended for demo)
- Navigate to Page 2
- Click "Train Model"
- Configure parameters in popover
- Watch progress in real-time

**Option 2: Via Command Line**
```bash
# Train with defaults
make train-model

# Train with auto-promotion (if F1 > 0.70)
make train-model-promote

# Or directly with custom parameters
python scripts/train_baseline_model.py --max-features 10000 --C 0.5 --auto-promote
```

---

## ğŸ” Monitoring & Drift Detection

### Prometheus Metrics

The API exposes metrics for monitoring:

```
rakuten_predictions_total - Total number of predictions
rakuten_prediction_latency_seconds - Prediction latency
rakuten_text_len_chars - Input text length distribution
rakuten_model_version - Current model version
```

### Grafana Dashboards

Pre-configured dashboards available at http://localhost:3000:

- **Model Performance**: Prediction counts, latency, error rates
- **Data Drift**: Input text length distribution over time
- **System Health**: API uptime, resource usage

### Inference Logging

All predictions are logged to `data/monitoring/inference_log.csv`:

```csv
timestamp,designation,description,predicted_class,confidence,model_version
2026-02-14 10:30:15,"Product title","Description",10,0.89,1
```

View logs in Streamlit (Page 4) for drift analysis.

---

## ğŸ§¹ Cleanup

```bash
# Stop services (keep data)
make stop

# Complete cleanup (deletes all data and volumes)
make clean

# Remove backup files
rm -f backup_*.sql
```

---

## ğŸ“š Project Structure

```
sep25_cmlops_rakuten/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/               # Data loading and preprocessing
â”‚   â”‚   â”œâ”€â”€ schema.sql      # Database schema with audit trail
â”‚   â”‚   â”œâ”€â”€ loader.py       # Incremental data loader
â”‚   â”‚   â”œâ”€â”€ dataset_generator.py  # Balanced dataset creation
â”‚   â”‚   â””â”€â”€ db_init.py      # Database initialization
â”‚   â”œâ”€â”€ features/           # Feature extraction
â”‚   â”‚   â””â”€â”€ text_features.py
â”‚   â”œâ”€â”€ models/             # Model training and evaluation
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”‚   â””â”€â”€ model_registry.py
â”‚   â”œâ”€â”€ serve/              # FastAPI serving
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ routes.py
â”‚   â”‚   â”œâ”€â”€ model_loader.py
â”‚   â”‚   â””â”€â”€ inference_logger.py
â”‚   â””â”€â”€ monitoring/         # Drift detection
â”‚       â””â”€â”€ drift_detector.py
â”œâ”€â”€ streamlit_app/          # Interactive UI
â”‚   â”œâ”€â”€ Home.py
â”‚   â”œâ”€â”€ pages/              # 4 pipeline pages
â”‚   â”œâ”€â”€ managers/           # Pipeline executors
â”‚   â””â”€â”€ components/         # Reusable components
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train_baseline_model.py  # Training script
â”œâ”€â”€ monitoring/             # Prometheus config
â”œâ”€â”€ grafana/                # Grafana dashboards
â”œâ”€â”€ docker-compose.yml      # Simplified stack (no orchestration)
â”œâ”€â”€ Makefile                # Convenient commands
â””â”€â”€ requirements.txt        # Python dependencies
```

---

## ğŸ“ Certification Presentation Points

### What This Project Demonstrates

1. **Data Management**
   - Incremental loading with audit trail
   - Complete data lineage and reproducibility
   - Database-based versioning (no external tools needed)

2. **Experiment Tracking**
   - MLflow for all experiments
   - Parameterized runs
   - Artifact storage in MinIO

3. **Model Registry**
   - Versioned models
   - Stage-based promotion workflow
   - Automated promotion based on metrics

4. **Model Serving**
   - REST API with FastAPI
   - Health checks and monitoring
   - Automatic model reloading

5. **Monitoring & Observability**
   - Prometheus metrics collection
   - Grafana visualization
   - Inference logging for drift detection

6. **Reproducibility**
   - All training runs are reproducible via MLflow run_id
   - Database audit trail enables exact data state recovery
   - Hyperparameters and artifacts fully tracked

---

## ğŸ› Troubleshooting

| Problem | Solution |
|---------|----------|
| Services don't start | `make check-health` then `make restart` |
| PostgreSQL not accessible | `make logs-postgres` then `docker compose restart postgres` |
| API not responding | Check if model exists in MLflow registry |
| Streamlit import errors | `make install-streamlit` |
| "No data in database" error | Run `make init-db` first |

**Complete reset:**
```bash
make stop && make clean
make demo
```

---

## ğŸ“ Support

For questions about this MLOps certification project:
- Review the code documentation in `src/`
- Check Makefile commands with `make help`
- View logs with `make logs`

---

**ğŸ“ DataScientest MLOps Certification - September 2025**
